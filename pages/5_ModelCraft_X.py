import streamlit as st
import pandas as pd
import requests

# -------------------- CONFIG -------------------- #
st.set_page_config(
    page_title="ModelCraft-X",
    page_icon="ğŸ§ªğŸ§¬",
    layout="wide"
)

BACKEND_URL = "http://localhost:8000"

# -------------------- UI HEADER -------------------- #
st.title("ğŸ§ªğŸ§¬ ModelCraft-X")
st.subheader("Cross-Validated AutoML Benchmarking Framework (Backend Powered)")
st.caption("Pipeline-based | Explainable | Reproducible")

# -------------------- DATA UPLOAD -------------------- #
uploaded_file = st.file_uploader("ğŸ“¤ Upload CSV Dataset", type=["csv"])

if uploaded_file:
    data = pd.read_csv(uploaded_file)
    st.success("âœ… Dataset Loaded Successfully")

    st.write("### ğŸ“Š Dataset Preview")
    st.dataframe(data.head(), use_container_width=True)

    # -------------------- TARGET SELECTION -------------------- #
    target_col = st.selectbox("ğŸ¯ Select Target Column", data.columns)

    if st.button("ğŸš€ Run AutoML Benchmarking"):

        with st.spinner("â³ Running backend AutoML benchmarking..."):
            response = requests.post(
                f"{BACKEND_URL}/modelcraft/benchmark",
                json={
                    "data": data.to_dict(orient="records"),
                    "target": target_col
                }
            )

        if response.status_code != 200:
            st.error("âŒ Backend error occurred.")
            st.stop()

        result = response.json()

        # -------------------- TASK TYPE -------------------- #
        st.info(f"ğŸ§  Detected Task Type: **{result['task_type'].upper()}**")

        # -------------------- BENCHMARK RESULTS -------------------- #
        st.subheader("ğŸ“Š Cross-Validated Model Benchmarking")

        benchmark_df = pd.DataFrame(result["benchmark"])
        st.dataframe(benchmark_df, use_container_width=True)

        # -------------------- BEST MODEL -------------------- #
        st.subheader("ğŸ† Best Model Summary")

        col1, col2 = st.columns(2)
        col1.metric(result["metric"], round(result["final_score"], 4))
        col2.metric("Training Time (sec)", round(result["training_time"], 2))

        st.success(f"âœ… Best Model: **{result['best_model']}**")

        # -------------------- EXPERIMENT LOG -------------------- #
        st.subheader("ğŸ§¾ Experiment Summary")
        st.json(result["experiment_log"])

else:
    st.info("ğŸ‘† Upload a CSV file to start the AutoML benchmarking process")

