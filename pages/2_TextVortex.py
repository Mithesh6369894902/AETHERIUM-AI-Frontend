import streamlit as st
import requests
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image
import io

# ================= CONFIG ================= #
st.set_page_config(
    page_title="TextVortex",
    page_icon="ğŸŒªï¸",
    layout="wide"
)

BACKEND_URL = "http://localhost:8000"

st.title("ğŸŒªï¸ TextVortex â€” NLP Intelligence Engine (Backend Powered)")

page = st.sidebar.radio(
    "Select Module",
    [
        "ğŸ  Home",
        "ğŸ”  Tokenization",
        "ğŸ›‘ Stopwords Removal",
        "ğŸŒ± Stemming",
        "ğŸŒ¿ Lemmatization",
        "ğŸ”¢ N-Grams",
        "ğŸ”‘ Keyword Extraction",
        "ğŸ“Š Text Statistics",
        "ğŸ“ˆ Text Complexity",
        "â˜ï¸ Word Cloud"
    ]
)

# ================= INPUT ================= #
text = st.text_area("âœï¸ Enter text:", height=200)

def validate():
    if not text.strip():
        st.warning("âš ï¸ Please enter text.")
        return False
    return True

# ================= HOME ================= #
if page == "ğŸ  Home":
    st.markdown("""
    **TextVortex** is the NLP intelligence layer of Ã†THERIUM.
    
    All natural language processing operations are executed
    through a centralized backend to ensure scalability,
    reproducibility, and platform independence.
    """)

# ================= TOKENIZATION ================= #
elif page == "ğŸ”  Tokenization" and validate():
    res = requests.post(
        f"{BACKEND_URL}/text/tokenize",
        json={"text": text}
    ).json()

    st.subheader("Word Tokens")
    st.write(res["words"])

    st.subheader("Sentence Tokens")
    st.write(res["sentences"])

# ================= STOPWORDS ================= #
elif page == "ğŸ›‘ Stopwords Removal" and validate():
    res = requests.post(
        f"{BACKEND_URL}/text/stopwords",
        json={"text": text}
    ).json()
    st.write(res["tokens"])

# ================= STEMMING ================= #
elif page == "ğŸŒ± Stemming" and validate():
    res = requests.post(
        f"{BACKEND_URL}/text/stemming",
        json={"text": text}
    ).json()
    st.write(res["tokens"])

# ================= LEMMATIZATION ================= #
elif page == "ğŸŒ¿ Lemmatization" and validate():
    res = requests.post(
        f"{BACKEND_URL}/text/lemmatization",
        json={"text": text}
    ).json()
    st.write(res["tokens"])

# ================= N-GRAMS ================= #
elif page == "ğŸ”¢ N-Grams" and validate():
    n = st.slider("Select N", 1, 4, 2)
    res = requests.post(
        f"{BACKEND_URL}/text/ngrams",
        json={"text": text, "n": n}
    ).json()
    st.write(res["ngrams"])

# ================= KEYWORDS ================= #
elif page == "ğŸ”‘ Keyword Extraction" and validate():
    res = requests.post(
        f"{BACKEND_URL}/text/keywords",
        json={"text": text}
    ).json()

    df = pd.DataFrame(res["keywords"])
    st.dataframe(df)

# ================= TEXT STATISTICS ================= #
elif page == "ğŸ“Š Text Statistics" and validate():
    res = requests.post(
        f"{BACKEND_URL}/text/statistics",
        json={"text": text}
    ).json()
    st.json(res)

# ================= TEXT COMPLEXITY ================= #
elif page == "ğŸ“ˆ Text Complexity" and validate():
    res = requests.post(
        f"{BACKEND_URL}/text/complexity",
        json={"text": text}
    ).json()
    st.json(res)

# ================= WORD CLOUD ================= #
elif page == "â˜ï¸ Word Cloud" and validate():
    res = requests.post(
        f"{BACKEND_URL}/text/wordcloud",
        json={"text": text}
    )

    img = Image.open(io.BytesIO(res.content))
    st.image(img, caption="Word Cloud", use_column_width=True)


